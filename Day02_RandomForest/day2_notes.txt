RANDOM FOREST IMPLEMENTATION
=============================

==========================
1. What is Random Forest?
==========================
    - Ensemble learning algorithm based on Bagging.
    - Builds multiple Decision Trees and aggregates their results using 
    - majority voting (for classification) or averaging (for regression).

    - Key Idea: 
        - Many weak learners -> strong learner
        - Helps reduce overfitting and increase accuracy.

====================================
2. Why Random Forest performs well?
====================================
    - Handles non-linearity.
    - Works well even with noisy or incomplete data.
    - Reduces Variance (Bagging technique).
    - Less Sensitive to Outliers.
    - Avoids overfitting by averaging many trees.

===========================
3. Steps in Random Forest:
===========================
    - Draw random samples from dataset (with replacement).
    - Build many decision trees on this sample.
    - At each split, choose random subset of features.
    - Final prediction: majority vote / average.

=======================
4. Evaluation Metrics:
=======================
    - Accuracy: Fraction of total correct predictions.
    - F1-Score (weighted): Harmonic mean of precision and recall.
    - Confusion matrix: Table showing True vs Predicted values.

=====================
5. Saving the Model:
=====================
    - Use joblib.dump(model, filename) to save trained model.
    - Later, load it using joblib.load(filename).

=================
6. output Files:
=================
    - random_forest_model.pkl - Trained Model
    - confusion_matrix_rf.png - Performance Visualization
    - Classification Report printed in console